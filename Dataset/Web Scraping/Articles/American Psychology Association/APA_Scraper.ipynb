{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ab2151-4bbf-411a-a471-5e9209298a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sitemap] GET https://www.apa.org/sitemap.xml -> 200 (application/xml; charset=utf-8)\n",
      "[sitemap] topics sitemaps found: 0\n",
      "[result] Total Topics URLs from topics sitemaps: 0\n"
     ]
    }
   ],
   "source": [
    "# --- FAST, CORRECT SITEMAP LOOP: only \"topics\" XML sitemaps ---\n",
    "import requests, itertools, time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"application/xml,text/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "INDEX_URL = \"https://www.apa.org/sitemap.xml\"\n",
    "\n",
    "def fetch_xml(url: str) -> BeautifulSoup:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30, allow_redirects=True)\n",
    "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    print(f\"[sitemap] GET {url} -> {r.status_code} ({ctype})\")\n",
    "    r.raise_for_status()\n",
    "    # only proceed if it really looks like XML\n",
    "    if \"xml\" not in ctype:\n",
    "        raise ValueError(f\"Not XML: {url} ({ctype})\")\n",
    "    return BeautifulSoup(r.text, \"xml\")\n",
    "\n",
    "# 1) Load the sitemap index\n",
    "idx = fetch_xml(INDEX_URL)\n",
    "\n",
    "# 2) Extract ONLY child sitemap URLs that end with .xml, then keep just those with \"topics\"\n",
    "child_maps = [\n",
    "    loc.get_text(strip=True)\n",
    "    for loc in idx.find_all(\"loc\")\n",
    "    if loc.get_text(strip=True).lower().endswith(\".xml\")\n",
    "]\n",
    "child_maps = [u for u in child_maps if \"topics\" in u.lower()]\n",
    "\n",
    "print(f\"[sitemap] topics sitemaps found: {len(child_maps)}\")\n",
    "for u in itertools.islice(child_maps, 0, 10):\n",
    "    print(\" -\", u)\n",
    "\n",
    "# Optional: limit while testing so it runs fast\n",
    "# child_maps = child_maps[:5]\n",
    "\n",
    "# 3) Visit each topics sitemap and collect /topics URLs\n",
    "all_topics = []\n",
    "for sm in child_maps:\n",
    "    try:\n",
    "        sm_soup = fetch_xml(sm)\n",
    "        for loc in sm_soup.find_all(\"loc\"):\n",
    "            u = loc.get_text(strip=True)\n",
    "            if \"/topics\" in u.lower() and not u.lower().endswith(\n",
    "                (\".pdf\",\".jpg\",\".png\",\".gif\",\".svg\",\".jpeg\",\".webp\",\".avif\")\n",
    "            ):\n",
    "                all_topics.append(u)\n",
    "        time.sleep(0.2)  # be polite\n",
    "    except Exception as e:\n",
    "        print(\"[warn] skipped:\", sm, \"->\", e)\n",
    "\n",
    "# 4) Deduplicate preserving order\n",
    "seen, deduped = set(), []\n",
    "for u in all_topics:\n",
    "    if u not in seen:\n",
    "        seen.add(u)\n",
    "        deduped.append(u)\n",
    "\n",
    "print(f\"[result] Total Topics URLs from topics sitemaps: {len(deduped)}\")\n",
    "for u in itertools.islice(deduped, 0, 10):\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a37144-1f52-4852-a685-4b96143e9326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
