{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc032a3e-b770-4473-b5ea-c3c978f225be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet requests beautifulsoup4 lxml pandas\n",
    "\n",
    "import requests, time, re, itertools\n",
    "from urllib.parse import urljoin, urldefrag, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a2c3ec8-b9c8-4a32-a336-6792c95d06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://mmha.org.my\"\n",
    "\n",
    "HEADERS_HTML = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "HEADERS_XML = {\n",
    "    **HEADERS_HTML,\n",
    "    \"Accept\": \"application/xml,text/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "def fetch(url, headers, timeout=30):\n",
    "    r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    print(f\"[fetch] {r.status_code} {r.url} ({ctype})\")\n",
    "    r.raise_for_status()\n",
    "    return r.text, ctype, r\n",
    "\n",
    "def soupify(html, kind=\"html\"):\n",
    "    parser = \"xml\" if kind==\"xml\" else \"lxml\"\n",
    "    try:\n",
    "        return BeautifulSoup(html, parser)\n",
    "    except Exception:\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def norm(u, base):\n",
    "    if not u: return None\n",
    "    u = urljoin(base, u)\n",
    "    u, _ = urldefrag(u)\n",
    "    return u\n",
    "\n",
    "def same_site(u):\n",
    "    return u and u.lower().startswith(BASE)\n",
    "\n",
    "def looks_like_article(u: str) -> bool:\n",
    "    \"\"\"Generic heuristic for MMHA pages.\"\"\"\n",
    "    if not same_site(u): return False\n",
    "    low = u.lower()\n",
    "    if any(low.endswith(ext) for ext in (\".pdf\",\".jpg\",\".jpeg\",\".png\",\".gif\",\".svg\",\".webp\",\".avif\",\".mp4\",\".json\",\".xml\",\".zip\",\".ico\")):\n",
    "        return False\n",
    "    path = urlparse(low).path.rstrip(\"/\")\n",
    "    # Accept paths that look like posts: /category/..., /2024/.., /post-slug/, /news/slug, etc.\n",
    "    segs = path.strip(\"/\").split(\"/\")\n",
    "    if len(segs) >= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_text(soup: BeautifulSoup) -> str:\n",
    "    for sel in [\"article\", \"main\", \"[role='main']\", \".entry-content\", \".post-content\", \".content\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            return el.get_text(\" \", strip=True)\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def detect_tags(blob: str) -> str:\n",
    "    tags = []\n",
    "    if re.search(r\"\\bdepress\", blob, re.I): tags.append(\"depression\")\n",
    "    if re.search(r\"\\banxiet|panic|phobi\", blob, re.I): tags.append(\"anxiety\")\n",
    "    return \", \".join(sorted(set(tags))) or \"\"\n",
    "\n",
    "YOUTH_RE = re.compile(r\"youth|teen|adolescent|student|college|young adult|young people|school|parent\", re.I)\n",
    "def detect_audience(blob: str) -> str:\n",
    "    return \"youth\" if YOUTH_RE.search(blob) else \"general public\"\n",
    "\n",
    "def polite_sleep(s=0.3):\n",
    "    time.sleep(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aeddba-f311-491f-88ea-f874cf6213e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch] 404 https://mmha.org.my/sitemap.xml (text/html; charset=utf-8)\n",
      "[sitemap] failed: 404 Client Error: Not Found for url: https://mmha.org.my/sitemap.xml\n",
      "Sitemap candidates: 0\n",
      "[fallback] Using HTML crawl.\n",
      "[fetch] 200 https://mmha.org.my/ (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/contact-us (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/donate-now (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/ (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/find-help (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/get-involved (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/education-training/mhfa-info (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/education-training/mhfa-courses (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/education-training/caregiver-programme-cse (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/media-resources (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/about-mmha (text/html; charset=utf-8)\n",
      "[fetch] 404 https://mmha.org.my/education-training/mmha-info (text/html; charset=utf-8)\n",
      "[crawl-skip] https://mmha.org.my/education-training/mmha-info -> 404 Client Error: Not Found for url: https://mmha.org.my/education-training/mmha-info\n",
      "[fetch] 404 https://mmha.org.my/education-training/mental-health-first-aid-course (text/html; charset=utf-8)\n",
      "[crawl-skip] https://mmha.org.my/education-training/mental-health-first-aid-course -> 404 Client Error: Not Found for url: https://mmha.org.my/education-training/mental-health-first-aid-course\n",
      "[fetch] 200 https://mmha.org.my/article-listing/bahasa-malaysia/obsesif-kompulsif-ocd (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/bahasa-malaysia/cara-rawatan-penyakit-mental (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/what-is-mental-health (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/in-loving-memory-sanchita-islam (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/depression (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/on-mental-health-awareness-in-the-muslim-community (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/healthy-emotion-regulation-can-reduce-anxiety-study-finds (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/sibling-bullying-linked-to-poor-mental-health (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/mental-health-in-schools-a-role-for-school-resource-officers (text/html; charset=utf-8)\n",
      "[fetch] 200 https://mmha.org.my/article-listing/english/naming-the-beast-mental-illness (text/html; charset=utf-8)\n",
      "[fetch] 404 https://mmha.org.my/downloads/newsletter/mental-health-bulletin-2019june (text/html; charset=utf-8)\n",
      "[crawl-skip] https://mmha.org.my/downloads/newsletter/mental-health-bulletin-2019june -> 404 Client Error: Not Found for url: https://mmha.org.my/downloads/newsletter/mental-health-bulletin-2019june\n",
      "[fetch] 200 https://mmha.org.my/downloads/newsletter/mental-health-bulletin-2019oct.pdf (application/pdf)\n"
     ]
    }
   ],
   "source": [
    "def discover_via_sitemap(base=BASE, limit_maps=10):\n",
    "    urls = []\n",
    "    try:\n",
    "        html, ctype, _ = fetch(base + \"/sitemap.xml\", HEADERS_XML)\n",
    "        if \"xml\" not in ctype:\n",
    "            print(\"[sitemap] Not XML, skipping.\")\n",
    "            return []\n",
    "        idx = soupify(html, \"xml\")\n",
    "        # gather child sitemap xmls\n",
    "        child_maps = [loc.get_text(strip=True) for loc in idx.find_all(\"loc\")]\n",
    "        child_maps = [u for u in child_maps if u.lower().endswith(\".xml\")]\n",
    "        print(\"[sitemap] child maps:\", len(child_maps))\n",
    "        # optional trim for speed\n",
    "        child_maps = child_maps[:limit_maps]\n",
    "        for sm in child_maps:\n",
    "            try:\n",
    "                sm_html, sm_ctype, _ = fetch(sm, HEADERS_XML)\n",
    "                if \"xml\" not in sm_ctype: \n",
    "                    continue\n",
    "                sm_soup = soupify(sm_html, \"xml\")\n",
    "                for loc in sm_soup.find_all(\"loc\"):\n",
    "                    u = loc.get_text(strip=True)\n",
    "                    if looks_like_article(u):\n",
    "                        urls.append(u)\n",
    "                polite_sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(\"[sitemap-skip]\", sm, \"->\", e)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(\"[sitemap] failed:\", e)\n",
    "    # dedupe\n",
    "    seen, deduped = set(), []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u); deduped.append(u)\n",
    "    return deduped\n",
    "\n",
    "def discover_via_html(start=BASE, max_pages=120):\n",
    "    \"\"\"Breadth-first crawl from homepage; stop after collecting enough pages.\"\"\"\n",
    "    queue = [start]\n",
    "    seen = set(queue)\n",
    "    found = []\n",
    "    while queue and len(seen) < max_pages:\n",
    "        cur = queue.pop(0)\n",
    "        try:\n",
    "            html, ctype, resp = fetch(cur, HEADERS_HTML)\n",
    "            if \"html\" not in ctype: \n",
    "                continue\n",
    "            soup = soupify(html, \"html\")\n",
    "        except Exception as e:\n",
    "            print(\"[crawl-skip]\", cur, \"->\", e); \n",
    "            continue\n",
    "        # collect article-like\n",
    "        for a in soup.select(\"a[href]\"):\n",
    "            u = norm(a.get(\"href\"), resp.url)\n",
    "            if not same_site(u): \n",
    "                continue\n",
    "            if u not in seen:\n",
    "                seen.add(u)\n",
    "                # push to queue a limited set of internal pages to keep discovery going\n",
    "                if len(queue) < max_pages:\n",
    "                    queue.append(u)\n",
    "                if looks_like_article(u):\n",
    "                    found.append(u)\n",
    "        polite_sleep(0.1)\n",
    "    # dedupe\n",
    "    seen2, deduped = set(), []\n",
    "    for u in found:\n",
    "        if u not in seen2:\n",
    "            seen2.add(u); deduped.append(u)\n",
    "    return deduped\n",
    "\n",
    "# Run discovery\n",
    "candidates = discover_via_sitemap(limit_maps=12)\n",
    "print(\"Sitemap candidates:\", len(candidates))\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    print(\"[fallback] Using HTML crawl.\")\n",
    "    candidates = discover_via_html(max_pages=150)\n",
    "\n",
    "print(\"Total candidates discovered:\", len(candidates))\n",
    "for u in itertools.islice(candidates, 0, 20):\n",
    "    print(\" -\", u)\n",
    "\n",
    "# For quick testing:\n",
    "# candidates = candidates[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55702e-485f-4f37-9324-0e2935a2bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(url: str) -> dict | None:\n",
    "    try:\n",
    "        html, ctype, _ = fetch(url, HEADERS_HTML)\n",
    "        if \"html\" not in ctype:\n",
    "            return None\n",
    "        soup = soupify(html, \"html\")\n",
    "\n",
    "        title_el = soup.find(\"h1\") or soup.find(\"title\")\n",
    "        title = title_el.get_text(strip=True) if title_el else \"\"\n",
    "\n",
    "        desc_el = (soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                   or soup.find(\"meta\", attrs={\"property\": \"og:description\"}))\n",
    "        description = (desc_el.get(\"content\") or \"\").strip() if desc_el else \"\"\n",
    "\n",
    "        text = extract_text(soup)\n",
    "        snippet = \" \".join(text.split()[:1200])\n",
    "        blob = \" \".join([url, title, description, snippet])\n",
    "\n",
    "        return {\n",
    "            \"URL\": url,\n",
    "            \"Title\": title,\n",
    "            \"Description\": description,\n",
    "            \"Tags\": detect_tags(blob),\n",
    "            \"Tone\": \"supportive\",\n",
    "            \"Audience\": detect_audience(blob),\n",
    "            \"Source\": \"MMHA Malaysia\",\n",
    "            \"Locality (yes or no)\": \"yes\",\n",
    "            \"Country\": \"Malaysia\",\n",
    "            \"Relevance\": \"\",   # leave empty\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"[skip]\", url, \"->\", e)\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "for i, u in enumerate(candidates):\n",
    "    if i and i % 20 == 0:\n",
    "        polite_sleep(1.0)\n",
    "    polite_sleep(0.25)\n",
    "    rec = parse_article(u)\n",
    "    if rec:\n",
    "        rows.append(rec)\n",
    "\n",
    "df_mmha = pd.DataFrame(rows, columns=[\n",
    "    \"URL\",\"Title\",\"Description\",\"Tags\",\"Tone\",\"Audience\",\n",
    "    \"Source\",\"Locality (yes or no)\",\"Country\",\"Relevance\"\n",
    "])\n",
    "df_mmha.insert(0, \"ID\", range(1, len(df_mmha) + 1))\n",
    "\n",
    "print(\"Rows collected:\", len(df_mmha))\n",
    "df_mmha.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a7e66-a043-451f-8269-341c3c05c3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
